{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table to summarize results\n",
    "\n",
    "This notebook creates a table with the normalized NLLHs for different models. It expects to find pickled predictions for 10 folds with the following filenames:\n",
    "\n",
    " * ```<scenario_name>.floc.<dist_name>.<fold>.100.<model>.pkl```\n",
    "\n",
    "`eval_model.py` automatically creates such files. \n",
    "\n",
    "**NOTE:** The first cell contains some variables that need to be adjusted. Evaluating all scenarios might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import scipy.stats as scst\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tabulate\n",
    "\n",
    "# 1) Paths\n",
    "output_dir = \"../../DistNet_nn_pkl\"        # where the .pkl files live\n",
    "save_dir   = \"../results/\"  # where tables / figure go\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 2) Helper imports\n",
    "sys.path.append(\"../\")\n",
    "from helper import load_data, preprocess, data_source_release\n",
    "\n",
    "# 3) General settings\n",
    "mean_exp = [\"rf\", \"nn\"]\n",
    "sc_dict  = data_source_release.get_sc_dict()\n",
    "\n",
    "# 4) Distribution you want to fit\n",
    "floc_exp = [\"lognormal_distfit\",\n",
    "            \"lognormal_distfit.rf2\",\n",
    "            \"lognormal_distfit.rf\",\n",
    "            \"lognormal_nn\"]\n",
    "distribution_hndl = scst.distributions.lognorm\n",
    "\n",
    "# 5) Scenarios to evaluate (empty list = all)\n",
    "SCENARIOS = []    # or pick specific keys\n",
    "load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spear_swgcp.floc.lognormal_nn.3.1.pkl ../../DistNet_nn_pkl\n",
      "         spear_swgcp lognormal_nn 3 False\n",
      "clasp_factoring.floc.lognormal_nn.3.1.pkl ../../DistNet_nn_pkl\n",
      "     clasp_factoring lognormal_nn 3 False\n",
      "yalsat_swgcp.floc.lognormal_nn.1.1.pkl ../../DistNet_nn_pkl\n",
      "        yalsat_swgcp lognormal_nn 1 False\n",
      "yalsat_swgcp.floc.lognormal_nn.3.1.pkl ../../DistNet_nn_pkl\n",
      "        yalsat_swgcp lognormal_nn 3 False\n",
      "clasp_factoring.floc.lognormal_nn.1.1.pkl ../../DistNet_nn_pkl\n",
      "     clasp_factoring lognormal_nn 1 False\n",
      "yalsat_qcp.floc.lognormal_nn.8.1.pkl ../../DistNet_nn_pkl\n",
      "          yalsat_qcp lognormal_nn 8 False\n",
      "spear_swgcp.floc.lognormal_nn.1.1.pkl ../../DistNet_nn_pkl\n",
      "         spear_swgcp lognormal_nn 1 False\n",
      "spear_swgcp.floc.lognormal_nn.5.1.pkl ../../DistNet_nn_pkl\n",
      "         spear_swgcp lognormal_nn 5 False\n",
      "yalsat_swgcp.floc.lognormal_nn.7.1.pkl ../../DistNet_nn_pkl\n",
      "        yalsat_swgcp lognormal_nn 7 False\n",
      "spear_qcp.floc.lognormal_nn.8.1.pkl ../../DistNet_nn_pkl\n",
      "           spear_qcp lognormal_nn 8 False\n",
      "clasp_factoring.floc.lognormal_nn.5.1.pkl ../../DistNet_nn_pkl\n",
      "     clasp_factoring lognormal_nn 5 False\n",
      "clasp_factoring.floc.lognormal_nn.7.1.pkl ../../DistNet_nn_pkl\n",
      "     clasp_factoring lognormal_nn 7 False\n",
      "lpg-zeno.floc.lognormal_nn.9.1.pkl ../../DistNet_nn_pkl\n",
      "            lpg-zeno lognormal_nn 9 False\n",
      "yalsat_swgcp.floc.lognormal_nn.5.1.pkl ../../DistNet_nn_pkl\n",
      "        yalsat_swgcp lognormal_nn 5 False\n",
      "saps-CVVAR.floc.lognormal_nn.9.1.pkl ../../DistNet_nn_pkl\n",
      "          saps-CVVAR lognormal_nn 9 False\n",
      "spear_swgcp.floc.lognormal_nn.7.1.pkl ../../DistNet_nn_pkl\n",
      "         spear_swgcp lognormal_nn 7 False\n",
      "clasp_factoring.floc.lognormal_nn.0.1.pkl ../../DistNet_nn_pkl\n",
      "     clasp_factoring lognormal_nn 0 False\n",
      "yalsat_qcp.floc.lognormal_nn.9.1.pkl ../../DistNet_nn_pkl\n",
      "          yalsat_qcp lognormal_nn 9 False\n",
      "yalsat_swgcp.floc.lognormal_nn.2.1.pkl ../../DistNet_nn_pkl\n",
      "        yalsat_swgcp lognormal_nn 2 False\n",
      "spear_swgcp.floc.lognormal_nn.0.1.pkl ../../DistNet_nn_pkl\n",
      "         spear_swgcp lognormal_nn 0 False\n",
      "spear_swgcp.floc.lognormal_nn.2.1.pkl ../../DistNet_nn_pkl\n",
      "         spear_swgcp lognormal_nn 2 False\n",
      "yalsat_swgcp.floc.lognormal_nn.0.1.pkl ../../DistNet_nn_pkl\n",
      "        yalsat_swgcp lognormal_nn 0 False\n",
      "clasp_factoring.floc.lognormal_nn.2.1.pkl ../../DistNet_nn_pkl\n",
      "     clasp_factoring lognormal_nn 2 False\n",
      "saps-CVVAR.floc.lognormal_nn.8.1.pkl ../../DistNet_nn_pkl\n",
      "          saps-CVVAR lognormal_nn 8 False\n",
      "yalsat_swgcp.floc.lognormal_nn.4.1.pkl ../../DistNet_nn_pkl\n",
      "        yalsat_swgcp lognormal_nn 4 False\n",
      "clasp_factoring.floc.lognormal_nn.6.1.pkl ../../DistNet_nn_pkl\n",
      "     clasp_factoring lognormal_nn 6 False\n",
      "lpg-zeno.floc.lognormal_nn.8.1.pkl ../../DistNet_nn_pkl\n",
      "            lpg-zeno lognormal_nn 8 False\n",
      "spear_swgcp.floc.lognormal_nn.6.1.pkl ../../DistNet_nn_pkl\n",
      "         spear_swgcp lognormal_nn 6 False\n",
      "spear_swgcp.floc.lognormal_nn.4.1.pkl ../../DistNet_nn_pkl\n",
      "         spear_swgcp lognormal_nn 4 False\n",
      "clasp_factoring.floc.lognormal_nn.4.1.pkl ../../DistNet_nn_pkl\n",
      "     clasp_factoring lognormal_nn 4 False\n",
      "spear_qcp.floc.lognormal_nn.9.1.pkl ../../DistNet_nn_pkl\n",
      "           spear_qcp lognormal_nn 9 False\n",
      "yalsat_swgcp.floc.lognormal_nn.6.1.pkl ../../DistNet_nn_pkl\n",
      "        yalsat_swgcp lognormal_nn 6 False\n",
      "saps-CVVAR.floc.lognormal_nn.6.1.pkl ../../DistNet_nn_pkl\n",
      "          saps-CVVAR lognormal_nn 6 False\n",
      "spear_qcp.floc.lognormal_nn.5.1.pkl ../../DistNet_nn_pkl\n",
      "           spear_qcp lognormal_nn 5 False\n",
      "lpg-zeno.floc.lognormal_nn.6.1.pkl ../../DistNet_nn_pkl\n",
      "            lpg-zeno lognormal_nn 6 False\n",
      "yalsat_qcp.floc.lognormal_nn.1.1.pkl ../../DistNet_nn_pkl\n",
      "          yalsat_qcp lognormal_nn 1 False\n",
      "clasp_factoring.floc.lognormal_nn.8.1.pkl ../../DistNet_nn_pkl\n",
      "     clasp_factoring lognormal_nn 8 False\n",
      "spear_swgcp.floc.lognormal_nn.8.1.pkl ../../DistNet_nn_pkl\n",
      "         spear_swgcp lognormal_nn 8 False\n",
      "yalsat_qcp.floc.lognormal_nn.3.1.pkl ../../DistNet_nn_pkl\n",
      "          yalsat_qcp lognormal_nn 3 False\n",
      "lpg-zeno.floc.lognormal_nn.4.1.pkl ../../DistNet_nn_pkl\n",
      "            lpg-zeno lognormal_nn 4 False\n",
      "spear_qcp.floc.lognormal_nn.7.1.pkl ../../DistNet_nn_pkl\n",
      "           spear_qcp lognormal_nn 7 False\n",
      "saps-CVVAR.floc.lognormal_nn.4.1.pkl ../../DistNet_nn_pkl\n",
      "          saps-CVVAR lognormal_nn 4 False\n",
      "yalsat_swgcp.floc.lognormal_nn.8.1.pkl ../../DistNet_nn_pkl\n",
      "        yalsat_swgcp lognormal_nn 8 False\n",
      "lpg-zeno.floc.lognormal_nn.0.1.pkl ../../DistNet_nn_pkl\n",
      "            lpg-zeno lognormal_nn 0 False\n",
      "yalsat_qcp.floc.lognormal_nn.7.1.pkl ../../DistNet_nn_pkl\n",
      "          yalsat_qcp lognormal_nn 7 False\n",
      "spear_qcp.floc.lognormal_nn.3.1.pkl ../../DistNet_nn_pkl\n",
      "           spear_qcp lognormal_nn 3 False\n",
      "saps-CVVAR.floc.lognormal_nn.0.1.pkl ../../DistNet_nn_pkl\n",
      "          saps-CVVAR lognormal_nn 0 False\n",
      "saps-CVVAR.floc.lognormal_nn.2.1.pkl ../../DistNet_nn_pkl\n",
      "          saps-CVVAR lognormal_nn 2 False\n",
      "spear_qcp.floc.lognormal_nn.1.1.pkl ../../DistNet_nn_pkl\n",
      "           spear_qcp lognormal_nn 1 False\n",
      "yalsat_qcp.floc.lognormal_nn.5.1.pkl ../../DistNet_nn_pkl\n",
      "          yalsat_qcp lognormal_nn 5 False\n",
      "lpg-zeno.floc.lognormal_nn.2.1.pkl ../../DistNet_nn_pkl\n",
      "            lpg-zeno lognormal_nn 2 False\n",
      "yalsat_swgcp.floc.lognormal_nn.9.1.pkl ../../DistNet_nn_pkl\n",
      "        yalsat_swgcp lognormal_nn 9 False\n",
      "spear_qcp.floc.lognormal_nn.6.1.pkl ../../DistNet_nn_pkl\n",
      "           spear_qcp lognormal_nn 6 False\n",
      "saps-CVVAR.floc.lognormal_nn.5.1.pkl ../../DistNet_nn_pkl\n",
      "          saps-CVVAR lognormal_nn 5 False\n",
      "yalsat_qcp.floc.lognormal_nn.2.1.pkl ../../DistNet_nn_pkl\n",
      "          yalsat_qcp lognormal_nn 2 False\n",
      "lpg-zeno.floc.lognormal_nn.5.1.pkl ../../DistNet_nn_pkl\n",
      "            lpg-zeno lognormal_nn 5 False\n",
      "lpg-zeno.floc.lognormal_nn.7.1.pkl ../../DistNet_nn_pkl\n",
      "            lpg-zeno lognormal_nn 7 False\n",
      "yalsat_qcp.floc.lognormal_nn.0.1.pkl ../../DistNet_nn_pkl\n",
      "          yalsat_qcp lognormal_nn 0 False\n",
      "clasp_factoring.floc.lognormal_nn.9.1.pkl ../../DistNet_nn_pkl\n",
      "     clasp_factoring lognormal_nn 9 False\n",
      "saps-CVVAR.floc.lognormal_nn.7.1.pkl ../../DistNet_nn_pkl\n",
      "          saps-CVVAR lognormal_nn 7 False\n",
      "spear_qcp.floc.lognormal_nn.4.1.pkl ../../DistNet_nn_pkl\n",
      "           spear_qcp lognormal_nn 4 False\n",
      "spear_swgcp.floc.lognormal_nn.9.1.pkl ../../DistNet_nn_pkl\n",
      "         spear_swgcp lognormal_nn 9 False\n",
      "yalsat_qcp.floc.lognormal_nn.4.1.pkl ../../DistNet_nn_pkl\n",
      "          yalsat_qcp lognormal_nn 4 False\n",
      "lpg-zeno.floc.lognormal_nn.3.1.pkl ../../DistNet_nn_pkl\n",
      "            lpg-zeno lognormal_nn 3 False\n",
      "saps-CVVAR.floc.lognormal_nn.3.1.pkl ../../DistNet_nn_pkl\n",
      "          saps-CVVAR lognormal_nn 3 False\n",
      "spear_qcp.floc.lognormal_nn.0.1.pkl ../../DistNet_nn_pkl\n",
      "           spear_qcp lognormal_nn 0 False\n",
      "spear_qcp.floc.lognormal_nn.2.1.pkl ../../DistNet_nn_pkl\n",
      "           spear_qcp lognormal_nn 2 False\n",
      "saps-CVVAR.floc.lognormal_nn.1.1.pkl ../../DistNet_nn_pkl\n",
      "          saps-CVVAR lognormal_nn 1 False\n",
      "lpg-zeno.floc.lognormal_nn.1.1.pkl ../../DistNet_nn_pkl\n",
      "            lpg-zeno lognormal_nn 1 False\n",
      "yalsat_qcp.floc.lognormal_nn.6.1.pkl ../../DistNet_nn_pkl\n",
      "          yalsat_qcp lognormal_nn 6 False\n"
     ]
    }
   ],
   "source": [
    "res_dict = dict()\n",
    "for fl in os.listdir(output_dir):\n",
    "    if \".pkl\" not in fl:\n",
    "        # Select only one seed\n",
    "        continue\n",
    "    print(fl, output_dir)\n",
    "\n",
    "    fl_path = os.path.join(output_dir, fl)\n",
    "    \n",
    "    if os.path.isdir(fl_path):\n",
    "        continue\n",
    "    \n",
    "    if not \"pkl\" in fl_path:\n",
    "        continue\n",
    "    \n",
    "    if \"txt\" in fl_path or \"png\" in fl_path:\n",
    "        continue\n",
    "\n",
    "    if not os.path.isfile(fl_path):\n",
    "        fl_path = os.path.join(output_dir)\n",
    "\n",
    "    with open(fl_path, \"rb\") as fh:\n",
    "        pkl = pickle.load(fh)\n",
    "        train_pred = np.array(pkl[0])\n",
    "        val_pred = np.array(pkl[1])\n",
    "        add_info = pkl[2]\n",
    "        task = add_info[\"task\"]\n",
    "        model = add_info[\"model\"]\n",
    "        scenario = add_info[\"scenario\"]\n",
    "        fold = add_info[\"fold\"]\n",
    "\n",
    "        if \"nn\" in add_info[\"model\"] and add_info[\"task\"] == \"floc\":\n",
    "            print(\"%20s\" % scenario, model, fold, add_info[\"loaded\"])\n",
    "    if scenario not in res_dict:\n",
    "        res_dict[scenario] = dict()\n",
    "\n",
    "    if model not in floc_exp and model not in mean_exp:\n",
    "        continue\n",
    "\n",
    "    if task not in res_dict[scenario]:\n",
    "        res_dict[scenario][task] = dict()\n",
    "        \n",
    "    if model not in res_dict[scenario][task]:\n",
    "        res_dict[scenario][task][model] = dict()\n",
    "\n",
    "    res_dict[scenario][task][model][fold] = (train_pred, val_pred)\n",
    "\n",
    "if len(SCENARIOS) == 0:\n",
    "    SCENARIOS = list(res_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### clasp_factoring\n",
      "floc         lognormal_nn 10\n",
      "### lpg-zeno\n",
      "floc         lognormal_nn 10\n",
      "### saps-CVVAR\n",
      "floc         lognormal_nn 10\n",
      "### spear_qcp\n",
      "floc         lognormal_nn 10\n",
      "### spear_swgcp\n",
      "floc         lognormal_nn 10\n",
      "### yalsat_qcp\n",
      "floc         lognormal_nn 10\n",
      "### yalsat_swgcp\n",
      "floc         lognormal_nn 10\n"
     ]
    }
   ],
   "source": [
    "for scen in sorted(SCENARIOS):\n",
    "    print(\"### %s\" % scen)\n",
    "    try:\n",
    "        for model in sorted(res_dict[scen][\"mean\"]):\n",
    "            print(\"mean %20s\" % model, len(res_dict[scen][\"mean\"][model]))\n",
    "    except:\n",
    "        pass\n",
    "    for model in sorted(res_dict[scen][\"floc\"]):\n",
    "        print(\"floc %20s\" % model, len(res_dict[scen][\"floc\"][model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### spear_swgcp #####\n",
      "Train data loaded\n",
      "Test data loaded\n",
      "/Users/gurmehersinghpuri/Freiburg/DL_LAB/Project/DistNet/notebooks/../data/spear_smallworlds/features.txt\n",
      "(12499, 100) (12499, 100)\n",
      "Discarding 0 (12499) instances because of CRASHED\n",
      "Discarding 34 (12499) instances because of TIMEOUT\n",
      "Discarding 0 (12465) instances because not stated TIMEOUTS\n",
      "Discarding 0 (12465) instances because of constant features\n",
      "Discarding 1283 (12465) instances because of UNSAT\n",
      "(11182, 100)\n",
      "##### clasp_factoring #####\n",
      "Train data loaded\n",
      "Test data loaded\n",
      "/Users/gurmehersinghpuri/Freiburg/DL_LAB/Project/DistNet/notebooks/../data/clasp-3.0.4-p8_rand_factoring/features.txt\n",
      "(2000, 113) (2000, 100)\n",
      "Discarding 0 (2000) instances because of CRASHED\n",
      "Discarding 0 (2000) instances because of TIMEOUT\n",
      "Discarding 0 (2000) instances because not stated TIMEOUTS\n",
      "Discarding 0 (2000) instances because of constant features\n",
      "Discarding 0 (2000) instances because of UNSAT\n",
      "(2000, 113)\n",
      "##### yalsat_swgcp #####\n",
      "Train data loaded\n",
      "Test data loaded\n",
      "/Users/gurmehersinghpuri/Freiburg/DL_LAB/Project/DistNet/notebooks/../data/yalsat_smallworlds/features.txt\n",
      "(12499, 100) (12499, 100)\n",
      "Discarding 0 (12499) instances because of CRASHED\n",
      "Discarding 1317 (12499) instances because of TIMEOUT\n",
      "Discarding 0 (11182) instances because not stated TIMEOUTS\n",
      "Discarding 0 (11182) instances because of constant features\n",
      "Discarding 0 (11182) instances because of UNSAT\n",
      "(11182, 100)\n",
      "##### yalsat_qcp #####\n",
      "Train data loaded\n",
      "Test data loaded\n",
      "/Users/gurmehersinghpuri/Freiburg/DL_LAB/Project/DistNet/notebooks/../data/yalsat_qcp-hard/features.txt\n",
      "(11962, 100) (11962, 100)\n",
      "Discarding 0 (11962) instances because of CRASHED\n",
      "Discarding 215 (11962) instances because of TIMEOUT\n",
      "Discarding 0 (11747) instances because not stated TIMEOUTS\n",
      "Discarding 4 (11747) instances because of constant features\n",
      "Discarding 0 (11743) instances because of UNSAT\n",
      "(11743, 100)\n",
      "##### spear_qcp #####\n",
      "Train data loaded\n",
      "Test data loaded\n",
      "/Users/gurmehersinghpuri/Freiburg/DL_LAB/Project/DistNet/notebooks/../data/spear_qcp-hard/features.txt\n",
      "(11962, 100) (11962, 100)\n",
      "Discarding 0 (11962) instances because of CRASHED\n",
      "Discarding 3816 (11962) instances because of TIMEOUT\n",
      "Discarding 0 (8146) instances because not stated TIMEOUTS\n",
      "Discarding 4 (8146) instances because of constant features\n",
      "Discarding 70 (8142) instances because of UNSAT\n",
      "(8072, 100)\n",
      "##### lpg-zeno #####\n",
      "Train data loaded\n",
      "Test data loaded\n",
      "/Users/gurmehersinghpuri/Freiburg/DL_LAB/Project/DistNet/notebooks/../data/lpg-zenotravel/features.txt\n",
      "(4000, 305) (4000, 100)\n",
      "Discarding 0 (4000) instances because of CRASHED\n",
      "Discarding 1 (4000) instances because of TIMEOUT\n",
      "Discarding 0 (3999) instances because not stated TIMEOUTS\n",
      "Discarding 0 (3999) instances because of constant features\n",
      "Discarding 0 (3999) instances because of UNSAT\n",
      "(3999, 305)\n",
      "##### saps-CVVAR #####\n",
      "Train data loaded\n",
      "Could not find test data\n",
      "/Users/gurmehersinghpuri/Freiburg/DL_LAB/Project/DistNet/notebooks/../data/CP06_CV-VAR/features.txt\n",
      "(20000, 54) (20000, 100)\n",
      "Discarding 0 (20000) instances because of CRASHED\n",
      "Discarding 9989 (20000) instances because of TIMEOUT\n",
      "Discarding 0 (10011) instances because not stated TIMEOUTS\n",
      "Discarding 0 (10011) instances because of constant features\n",
      "Discarding 0 (10011) instances because of UNSAT\n",
      "(10011, 54)\n",
      "dumped to ../../DistNet_nn_pkl\n"
     ]
    }
   ],
   "source": [
    "nllh_dict = dict()\n",
    "mean_dict = dict()\n",
    "var_dict = dict()\n",
    "if load:\n",
    "    with open(save_dir + \"/\" + distribution_hndl.name + \".nllh.pkl\", \"rb\") as fh:\n",
    "        nllh_dict = pickle.load(fh)\n",
    "        \n",
    "    with open(save_dir + \"/\" + distribution_hndl.name + \".mean.pkl\", \"rb\") as fh:\n",
    "        mean_dict = pickle.load(fh)\n",
    "        \n",
    "    with open(save_dir + \"/\" + distribution_hndl.name + \".var.pkl\", \"rb\") as fh:\n",
    "        var_dict = pickle.load(fh)\n",
    "    print(\"LOADED\")\n",
    "else:\n",
    "    for scen in SCENARIOS:\n",
    "        print(\"##### %s #####\" % scen)\n",
    "        nllh_dict[scen] = dict()\n",
    "        mean_dict[scen] = {\"true\": {\"train\": [], \"val\": []}, \"floc\": dict(), \"mean\": dict()}\n",
    "        var_dict[scen] = {\"true\": {\"train\": [], \"val\": []}, \"floc\": dict(), \"mean\": dict()}\n",
    "\n",
    "        # 1) Load data\n",
    "        sc_dict = data_source_release.get_sc_dict()\n",
    "        data_dir = data_source_release.get_data_dir()\n",
    "\n",
    "        runtimes, features, sat_ls = load_data.\\\n",
    "            get_data(scenario=scen, data_dir=data_dir,\n",
    "                     sc_dict=sc_dict, retrieve=sc_dict[scen]['use'])\n",
    "\n",
    "        # We do not need any features\n",
    "        del features\n",
    "\n",
    "        # Get CV splits\n",
    "        idx = list(range(runtimes.shape[0]))\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "        fold = -1\n",
    "        for train, valid in kf.split(idx):\n",
    "            fold += 1\n",
    "            #print(\"## Split %d\" % ctr)\n",
    "\n",
    "            y_tra_run = runtimes[train]\n",
    "            y_val_run = runtimes[valid]\n",
    "\n",
    "            y_max_ = np.max(y_tra_run)\n",
    "            y_min_ = 0\n",
    "            y_tra_run = (y_tra_run - y_min_) / y_max_\n",
    "            y_val_run = (y_val_run - y_min_) / y_max_\n",
    "\n",
    "            task = \"floc\"\n",
    "            for model in floc_exp:\n",
    "                if model not in res_dict[scen][task]:\n",
    "                    continue\n",
    "\n",
    "                if model not in nllh_dict[scen]:\n",
    "                    nllh_dict[scen][model] = np.zeros([10, 2]) * np.nan\n",
    "                    mean_dict[scen][task][model] = {\"train\": list(), \"val\": list()}\n",
    "                    var_dict[scen][task][model] = {\"train\": list(), \"val\": list()}\n",
    "\n",
    "                if fold not in res_dict[scen][task][model]:\n",
    "                    print(\"FAILED, %s, %s, %s, fold %d\" % (scen, task, model, fold))\n",
    "                    continue\n",
    "\n",
    "                for observations, data, idx in ([y_tra_run, \"train\", 0], [y_val_run, \"val\", 1]):\n",
    "                    assert observations.shape[0] == res_dict[scen][task][model][fold][idx].shape[0]\n",
    "                    nllh = list()\n",
    "                    mean = list()\n",
    "                    var = list()\n",
    "                    for obs, p in zip(observations, res_dict[scen][task][model][fold][idx]):\n",
    "                        if distribution_hndl.name == \"expon\":\n",
    "                            if type(p) == np.float64: \n",
    "                                p = [p, ]\n",
    "                            if len(p) == 1: p = [0, p[0]]\n",
    "                            assert p[0] == 0\n",
    "                        else:\n",
    "                            if len(p) == 2: p = [p[0], 0, p[1]]\n",
    "                            assert p[1] == 0\n",
    "                        nllh_per_inst = distribution_hndl.logpdf(obs, *p[:-2], loc=p[-2], scale=p[-1]) + np.log(max(obs))              \n",
    "                        nllh_per_inst = np.mean(-nllh_per_inst)\n",
    "                        nllh.append(nllh_per_inst)\n",
    "                        mean.append(distribution_hndl.mean(*p[:-2], loc=p[-2], scale=p[-1]))\n",
    "                        var.append(distribution_hndl.var(*p[:-2], loc=p[-2], scale=p[-1]))\n",
    "                    nllh_dict[scen][model][fold, idx] = np.mean(nllh)\n",
    "                    mean_dict[scen][task][model][data].append(mean)\n",
    "                    var_dict[scen][task][model][data].append(var)\n",
    "\n",
    "            task = \"mean\"\n",
    "            if task not in res_dict[scen]:\n",
    "                continue\n",
    "            for model in mean_exp:\n",
    "                if model not in res_dict[scen][task]:\n",
    "                    continue\n",
    "                if model not in mean_dict[scen][task]:\n",
    "                    mean_dict[scen][task][model] = {\"train\": list(), \"val\": list()}\n",
    "                    var_dict[scen][task][model] = {\"train\": list(), \"val\": list()}\n",
    "                if fold not in res_dict[scen][task][model]:\n",
    "                    print(\"FAILED, %s, %s, %s, fold %d\" % (scen, task, model, fold))\n",
    "                    continue\n",
    "\n",
    "                for observations, data, idx in ([y_tra_run, \"train\", 0], [y_val_run, \"val\", 1]):\n",
    "                    assert observations.shape[0] == len(res_dict[scen][task][model][fold][idx]), (model, scenario, y_tra_run.shape[0], len(res_dict[scen][task][model][fold][idx]))\n",
    "                    mean_ls = list()\n",
    "                    tr_mean = list()\n",
    "                    tr_var = list()\n",
    "                    for obs, mean in zip(observations, res_dict[scen][task][model][fold][idx]):\n",
    "                        mean_ls.append(mean)\n",
    "                        tr_mean.append(np.mean(obs))\n",
    "                        tr_var.append(np.var(obs, ddof=1))\n",
    "\n",
    "                    mean_dict[scen][task][model][data].append(mean_ls)\n",
    "                    if len(mean_dict[scen][\"true\"][data]) == fold:\n",
    "                        mean_dict[scen][\"true\"][data].append(tr_mean)\n",
    "                        var_dict[scen][\"true\"][data].append(tr_var)\n",
    "    with open(save_dir + \"/\" + distribution_hndl.name + \".nllh.pkl\", \"wb\") as fh:\n",
    "        pickle.dump(nllh_dict, fh)\n",
    "\n",
    "    with open(save_dir + \"/\" + distribution_hndl.name + \".mean.pkl\", \"wb\") as fh:\n",
    "        pickle.dump(mean_dict, fh)\n",
    "\n",
    "    with open(save_dir + \"/\" + distribution_hndl.name + \".var.pkl\", \"wb\") as fh:\n",
    "        pickle.dump(var_dict, fh)\n",
    "    print(\"dumped to \" + output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc                 lognormal_nn\n",
      "---------------  --------------\n",
      "clasp_factoring          -0.264\n",
      "                         -0.084\n",
      "lpg-zeno                 -0.860\n",
      "                         -0.855\n",
      "saps-CVVAR               -0.673\n",
      "                         -0.606\n",
      "spear_qcp                -1.129\n",
      "                         -1.106\n",
      "spear_swgcp              -0.528\n",
      "                         -0.455\n",
      "yalsat_qcp               -0.767\n",
      "                         -0.764\n",
      "yalsat_swgcp             -0.824\n",
      "                         -0.800\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tabulate\n",
    "\n",
    "# (your existing nllh_dict & floc_exp code here…)\n",
    "\n",
    "# Build full table as before\n",
    "tab_data = []\n",
    "header   = [\"sc\"] + floc_exp  # e.g. [\"sc\", \"invgauss_nn\", \"lognormal_nn\", \"expon_nn\", ...]\n",
    "\n",
    "for scen in sorted(nllh_dict):\n",
    "    dummy = np.ones([2,2]) * np.inf\n",
    "    # row[0] = train NLLH mean; row[1] = test NLLH mean for each model\n",
    "    tab_data.append([scen] + [ np.mean(nllh_dict[scen].get(m, dummy)[:,0]) for m in header[1:] ])\n",
    "    tab_data.append([\"\"]   + [ np.mean(nllh_dict[scen].get(m, dummy)[:,1]) for m in header[1:] ])\n",
    "\n",
    "# Now pick out only the DistNet column:\n",
    "distnet_col = \"lognormal_nn\"            # change to whatever your model key is\n",
    "idx = header.index(distnet_col)\n",
    "\n",
    "# Build a reduced table with only “sc” and that one column\n",
    "new_header = [\"sc\", distnet_col]\n",
    "new_data   = []\n",
    "for row in tab_data:\n",
    "    new_data.append([ row[0], row[idx] ])  # row[0]=scenario name, row[idx]=DistNet value\n",
    "\n",
    "# Print & save\n",
    "out = tabulate.tabulate(new_data, headers=new_header, floatfmt=\"5.3f\")\n",
    "print(out)\n",
    "\n",
    "with open(f\"{save_dir}/nllh_{distribution_hndl.name}_distnet_only.txt\", \"w\") as fh:\n",
    "    fh.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
